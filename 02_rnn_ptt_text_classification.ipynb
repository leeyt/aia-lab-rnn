{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import word2vec\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.1 Load article cutted and article df and define y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/article_cutted\", \"rb\") as file:\n",
    "    docs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/article_preprocessed.csv')\n",
    "\n",
    "diff_threshold = 20\n",
    "df = df[abs(df['push']-df['boo']) > diff_threshold].copy()\n",
    "df['type'] = np.clip(df['push']-df['boo'], 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.2 Create word <-> id mappings and word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = word2vec.Word2Vec.load('word2vec_model/CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id = { k:i for i, k in enumerate(w2v.wv.vocab.keys()) }\n",
    "id2word = { i:k for k, i in word2id.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_len = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = np.zeros((words_len+2, 256))\n",
    "for k, v in word2id.items():\n",
    "    embedding[v] = w2v.wv[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.3 Transform sentence to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_length = 80\n",
    "docs_id = []\n",
    "for doc in docs:\n",
    "    text = doc[:input_length]\n",
    "    ids = [words_len]*input_length\n",
    "    ids[:len(text)] = [ word2id[w] if w in word2id else words_len+1 for w in text ]\n",
    "    \n",
    "    docs_id.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['韓瑜', '協志', '前妻', '正', '女演員', '周子', '瑜', 'TWICE', '團裡裡面', '台灣', '人', '正', '兩個', '要當', '鄉民', '老婆', '選', '五樓', '真', '勇氣']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 100035, 8, 9, 3, 10, 11, 12, 13, 14, 15, 16, 17, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034, 100034]\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])\n",
    "print(docs_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1 Create Training and Testing sets and create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, shuffle=True, stratify=df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_generator(df, batch_size, docs_id):\n",
    "    dfs = [ sub_df for key, sub_df in df.groupby('type')]\n",
    "    df_n = len(dfs)\n",
    "    \n",
    "    docs_id = np.array(docs_id)\n",
    "    while True:\n",
    "        selected = pd.concat([ sub_df.sample(int(batch_size/2)) for sub_df in dfs ], axis=0)\n",
    "        selected = selected.sample(frac=1)\n",
    "        x = docs_id[selected['idx']]\n",
    "        y = np.array(selected['type'].tolist()).reshape((batch_size, 1))\n",
    "                    \n",
    "        yield x, y\n",
    "        \n",
    "def test_data_generator(df, docs_id):\n",
    "    docs_id = np.array(docs_id)\n",
    "    x = docs_id[df['idx']]\n",
    "    y = np.array(df['type'].tolist()).reshape((len(x), 1))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test = test_data_generator(test, docs_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "update_per_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_cell(hidden_layer_size, batch_size, number_of_layers, dropout=True, dropout_rate=0.8):\n",
    "    layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
    "    \n",
    "    if dropout:\n",
    "        layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "        \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([layer]*number_of_layers)\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_layer(lstm_output, in_size, out_size):\n",
    "    x = lstm_output[:, -1, :]\n",
    "    print(x)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.05), name='output_layer_weights')\n",
    "    bias = tf.Variable(tf.zeros([out_size]), name='output_layer_bias')\n",
    "    \n",
    "    output = tf.matmul(x, weights) + bias\n",
    "    output = tf.nn.sigmoid(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_loss(logits, targets, learning_rate, grad_clip_margin):\n",
    "    loss = tf.losses.sigmoid_cross_entropy(targets, logits)\n",
    "    \n",
    "    # Cliping the gradient loss\n",
    "    gradients = tf.gradients(loss, tf.trainable_variables())\n",
    "    clipper_, _ = tf.clip_by_global_norm(gradients, grad_clip_margin)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_optimizer = optimizer.apply_gradients(zip(clipper_, tf.trainable_variables()))\n",
    "    \n",
    "    return loss, train_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextClassificationRNN(object):\n",
    "    def __init__(self, learning_rate=0.001, hidden_layer_size=64, number_of_layers=1, dropout=True, \n",
    "                 dropout_rate=0.8, number_of_classes=1, gradient_clip_margin=4, input_length=input_length, wv=embedding):\n",
    "    \n",
    "        self.inputs = tf.placeholder(tf.int32, [None, input_length], name='input_data')\n",
    "        self.targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n",
    "        self.bz = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "        \n",
    "        ## embedding lookup table\n",
    "        em_W = tf.Variable(wv.astype(np.float32), trainable=True)\n",
    "        x = tf.nn.embedding_lookup(em_W, self.inputs)\n",
    "\n",
    "        cell, init_state = LSTM_cell(hidden_layer_size, self.bz, number_of_layers, dropout, dropout_rate)\n",
    "\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, x, initial_state=init_state)\n",
    "\n",
    "        self.logits = output_layer(outputs, hidden_layer_size, number_of_classes)\n",
    "\n",
    "        self.loss, self.opt = opt_loss(self.logits, self.targets, learning_rate, gradient_clip_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(?, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = TextClassificationRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(100035, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0' shape=(320, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'output_layer_weights:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'output_layer_bias:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'Variable/Adam:0' shape=(100035, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable/Adam_1:0' shape=(100035, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam:0' shape=(320, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam_1:0' shape=(320, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam_1:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'output_layer_weights/Adam:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'output_layer_weights/Adam_1:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'output_layer_bias/Adam:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'output_layer_bias/Adam_1:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session =  tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 16:29:31.367544 Epoch 0/100 Train loss: 0.6992793083190918 Train auc: 0.531796875 Test loss: 0.6531446576118469 Test auc: 0.5295907051510311\n",
      "2018-03-08 16:32:35.981090 Epoch 5/100 Train loss: 0.6408179998397827 Train auc: 0.7740234375 Test loss: 0.48110008239746094 Test auc: 0.6776090893367653\n",
      "2018-03-08 16:35:53.274726 Epoch 10/100 Train loss: 0.6273868083953857 Train auc: 0.757890625 Test loss: 0.47387897968292236 Test auc: 0.6928253858440754\n",
      "2018-03-08 16:39:19.684644 Epoch 15/100 Train loss: 0.6171111464500427 Train auc: 0.852265625 Test loss: 0.4186032712459564 Test auc: 0.6814764322267552\n",
      "2018-03-08 16:42:41.228812 Epoch 20/100 Train loss: 0.6226093173027039 Train auc: 0.78921875 Test loss: 0.3884423077106476 Test auc: 0.6897465179924918\n",
      "2018-03-08 16:46:02.989483 Epoch 25/100 Train loss: 0.5978072285652161 Train auc: 0.829140625 Test loss: 0.3968924582004547 Test auc: 0.6997207272283322\n",
      "2018-03-08 16:49:09.750765 Epoch 30/100 Train loss: 0.6052713990211487 Train auc: 0.81453125 Test loss: 0.38981303572654724 Test auc: 0.6884353603076578\n",
      "2018-03-08 16:52:01.518047 Epoch 35/100 Train loss: 0.5909310579299927 Train auc: 0.83705078125 Test loss: 0.40136778354644775 Test auc: 0.6865970689076315\n",
      "2018-03-08 16:54:58.247301 Epoch 40/100 Train loss: 0.5873870253562927 Train auc: 0.835390625 Test loss: 0.40915948152542114 Test auc: 0.6897630505336196\n",
      "2018-03-08 16:58:30.410694 Epoch 45/100 Train loss: 0.5828830599784851 Train auc: 0.8566796875 Test loss: 0.3893865942955017 Test auc: 0.669469992166119\n",
      "2018-03-08 17:02:11.419443 Epoch 50/100 Train loss: 0.5992297530174255 Train auc: 0.836328125 Test loss: 0.3838987648487091 Test auc: 0.6511354040553052\n",
      "2018-03-08 17:05:31.419745 Epoch 55/100 Train loss: 0.5779344439506531 Train auc: 0.844140625 Test loss: 0.4001099467277527 Test auc: 0.7073918263116663\n",
      "2018-03-08 17:08:47.305829 Epoch 60/100 Train loss: 0.5757041573524475 Train auc: 0.87306640625 Test loss: 0.38629254698753357 Test auc: 0.65669415307607\n",
      "2018-03-08 17:11:45.827302 Epoch 65/100 Train loss: 0.5794107913970947 Train auc: 0.87154296875 Test loss: 0.38256776332855225 Test auc: 0.6481633618540863\n",
      "2018-03-08 17:14:46.238160 Epoch 70/100 Train loss: 0.5914627909660339 Train auc: 0.8478125 Test loss: 0.3790472447872162 Test auc: 0.6446012351079956\n",
      "2018-03-08 17:17:49.095607 Epoch 75/100 Train loss: 0.5963535904884338 Train auc: 0.845390625 Test loss: 0.3799015283584595 Test auc: 0.6665119390381622\n",
      "2018-03-08 17:20:55.599060 Epoch 80/100 Train loss: 0.5856469869613647 Train auc: 0.855 Test loss: 0.38072413206100464 Test auc: 0.6630782574192957\n",
      "2018-03-08 17:24:17.308212 Epoch 85/100 Train loss: 0.58470219373703 Train auc: 0.8602734375 Test loss: 0.3800510764122009 Test auc: 0.6444842355861677\n",
      "2018-03-08 17:27:21.859123 Epoch 90/100 Train loss: 0.5868678092956543 Train auc: 0.850703125 Test loss: 0.45394542813301086 Test auc: 0.7029528390188319\n",
      "2018-03-08 17:30:25.850994 Epoch 95/100 Train loss: 0.5706586241722107 Train auc: 0.895625 Test loss: 0.3879208266735077 Test auc: 0.7152568648197698\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "train_generate = train_data_generator(train, batch_size, docs_id)\n",
    "\n",
    "train_loss = []\n",
    "train_auc = []\n",
    "test_loss = []\n",
    "test_auc = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trained_scores = []\n",
    "    epoch_loss = []\n",
    "    for j in range(update_per_epochs):\n",
    "        X_batch, y_batch = next(train_generate) \n",
    "        \n",
    "        o, c, _ = session.run([model.logits, model.loss, model.opt], feed_dict={\n",
    "            model.inputs: X_batch, \n",
    "            model.targets: y_batch,\n",
    "            model.bz: np.array(batch_size)\n",
    "        })\n",
    "        \n",
    "        epoch_loss.append(c)\n",
    "        trained_scores.append(roc_auc_score(y_batch, o))\n",
    "    \n",
    "    to, tc = session.run([model.logits, model.loss], feed_dict={\n",
    "        model.inputs: X_test, \n",
    "        model.targets: Y_test,\n",
    "        model.bz:np.array(len(X_test))\n",
    "    })\n",
    "    \n",
    "    train_loss.append(np.mean(epoch_loss))\n",
    "    train_auc.append(np.mean(trained_scores))\n",
    "    test_loss.append(tc)\n",
    "    test_auc.append(roc_auc_score(Y_test, to))\n",
    "    \n",
    "    if (i % 5) == 0:\n",
    "        print(str(datetime.now()),\n",
    "              'Epoch {}/{}'.format(i, epochs), \n",
    "              'Train loss: {}'.format(np.mean(epoch_loss)), \n",
    "              'Train auc: {}'.format(np.mean(trained_scores)), \n",
    "              'Test loss: {}'.format(tc), \n",
    "              'Test auc: {}'.format(roc_auc_score(Y_test, to)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5530333518981934 Train auc: 0.8741015625 Test loss: 0.43621957302093506 Test auc: 0.6846856528064625\n"
     ]
    }
   ],
   "source": [
    "# 顯示最後結果\n",
    "print('Train loss: {}'.format(np.mean(epoch_loss)), \n",
    "      'Train auc: {}'.format(np.mean(trained_scores)), \n",
    "      'Test loss: {}'.format(tc), \n",
    "      'Test auc: {}'.format(roc_auc_score(Y_test, to)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
